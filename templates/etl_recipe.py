# -*- coding: utf-8 -*-

"""{{ name }}"""

import os.path as osp
from datetime import datetime, timedelta

from airflow import DAG
from airflow.models import Variable
from airflow.operators import (GenerateDatapackageOperator,
                               GitCheckoutOperator, GitPushOperator,
                               RunETLOperator, ValidateDatasetOperator)
from airflow.operators.sensors import ExternalTaskSensor
from airflow.operators.subdag_operator import SubDagOperator

# steps:
# 1. checkout the airflow branch
# 2. run etl.py
# 3. generate datapackage
# 4. validate-ddf
# 4. if there are updates, push

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': {{ datetime }},
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}

target_dataset = 'open-numbers/{{ name }}'
depends_on = {{ dependencies }}

datasets_dir = Variable.get('datasets_dir')

out_dir = osp.join(datasets_dir, target_dataset)
dag_id = target_dataset.replace('/', '_')
sub_dag_id = dag_id + '.' + 'dependency_check'

# now define the DAG
dag = DAG(dag_id, default_args=default_args,
          schedule_interval='@daily')


def sub_dag():
    subdag = DAG(sub_dag_id, default_args=default_args, schedule_interval='@once')

    def get_time_delta(n):
        delta = n - datetime(n.year, n.month, n.day)
        return delta

    dep_tasks = []

    update_datasets = ExternalTaskSensor(task_id='update_datasets', dag=subdag,
                                         external_dag_id='update_all_datasets',
                                         external_task_id='update_all_dataset',
                                         execution_date_fn=get_time_delta)
    for dep in depends_on:
        t = ExternalTaskSensor(task_id='wait_for_{}'.format(dep).replace('/', '_'),
                               dag=subdag,
                               allowed_states=['success', 'failed'],
                               external_dag_id=dep.replace('/', '_'),
                               external_task_id='git_push')
        dep_tasks.append(t)

    return subdag


dependency_task = SubDagOperator(subdag=sub_dag(), task_id='dependency_check', dag=dag)

checkout_task = GitCheckoutOperator(task_id='checkout_airflow_branch', dag=dag,
                                    dataset=out_dir, version='autogenerated')
recipe_task = RunETLOperator(task_id='run_etl', dag=dag,
                             dataset=out_dir)
datapackage_task = GenerateDatapackageOperator(task_id='generate_datapackage', dag=dag,
                                               dataset=out_dir)
validate_ddf = ValidateDatasetOperator(task_id='validate', dag=dag,
                                       dataset=out_dir)
git_push_task = GitPushOperator(task_id='git_push', dag=dag,
                                dataset=out_dir)

# set dependencies
(dependency_task >>
 checkout_task >>
 recipe_task >>
 datapackage_task >>
 validate_ddf >>
 git_push_task)
