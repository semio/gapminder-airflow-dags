# -*- coding: utf-8 -*-

"""{{ name }}"""

import os
import os.path as osp
from datetime import datetime, timedelta

from airflow import DAG
from airflow.models import Variable
from airflow.hooks.base_hook import BaseHook
from airflow.operators.ddf_plugin import (GenerateDatapackageOperator,
                                          DependencyDatasetSensor,
                                          UpdateSourceOperator,
                                          GitCheckoutOperator, GitPushOperator,
                                          GitMergeOperator, RunETLOperator,
                                          GitResetOperator, CleanCFCacheOperator,
                                          GCSUploadOperator, ValidateDatasetOperator,
                                          SlackReportOperator)
from airflow.operators.subdag_operator import SubDagOperator
from airflow.executors.local_executor import LocalExecutor

from functools import partial
import logging

# steps:
# 1. checkout the airflow branch
# 2. merge from develop branch
# 3. run update_source.py
# 4. run etl.py
# 5. generate datapackage
# 6. validate-ddf
# 7. if there are updates, push


# variables
datasets_dir = Variable.get('datasets_dir')
airflow_home = Variable.get('airflow_home')
# gcs_datasets = [x.strip() for x in Variable.get('with_production').split('\n')]
endpoint = BaseHook.get_connection('slack_connection').password
airflow_baseurl = BaseHook.get_connection('airflow_web').host

target_dataset = '{{ name }}'
depends_on = {{ dependencies }}

logpath = osp.join(airflow_home, 'validation-log')
out_dir = osp.join(datasets_dir, target_dataset)
dag_id = target_dataset.replace('/', '_')
sub_dag_id = dag_id + '.' + 'dependency_check'


def slack_report(context, status):
    task = SlackReportOperator(task_id='slack_report', http_conn_id='slack_connection',
                               endpoint=endpoint, status=status, airflow_baseurl=airflow_baseurl)
    context['target_dataset'] = '{{ name }}'
    task.execute(context)


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': {{ datetime }},
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'etl',
    'priority_weight': {{ priority }},
    # 'end_date': datetime(2016, 1, 1),
    'poke_interval': 60 * 10,  # 10 minutes
    'execution_timeout': timedelta(hours=10),     # 10 hours
    'weight_rule': 'absolute',
    'on_failure_callback': partial(slack_report, status='failed')
}

# now define the DAG
etl_type = "{{ etl_type }}"

{% if etl_type == 'recipe' %}
schedule = '0 12 * * *'   # recipe datasets: 12:00 everyday
{% else %}
schedule = '0 1 * * 0'    # source datasets: 1:00 every Sunday
{% endif %}

dag = DAG(dag_id, default_args=default_args,
          schedule_interval=schedule)


def sub_dag():
    args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': {{ datetime }},
        'retry_delay': timedelta(minutes=5),
        'poke_interval': 60 * 10,
        'timeout': 60 * 60 * 8,
        'priority_weight': {{ priority }},
        'weight_rule': 'absolute',
        'pool': 'dependency_checking'
    }
    subdag = DAG(sub_dag_id, default_args=args, schedule_interval='@once')

    dep_tasks = []

    # if etl_type != 'recipe':
    #     update_datasets = DependencyDatasetSensor(task_id='update_datasets', dag=subdag,
    #                                               external_dag_id='update_all_datasets',
    #                                               external_task_id='refresh_dags')

    for dep in depends_on.keys():
        t = DependencyDatasetSensor(task_id='wait_for_{}'.format(dep).replace('/', '_'),
                                    dag=subdag,
                                    external_dag_id=dep.replace('/', '_'),
                                    external_task_id='validate')
        dep_tasks.append(t)

    return subdag


dependency_task = SubDagOperator(subdag=sub_dag(), task_id='dependency_check', dag=dag, executor=LocalExecutor(parallelism=2))

checkout_task = GitCheckoutOperator(task_id='checkout_autogen_branch', dag=dag,
                                    dataset=out_dir, branch='autogenerated')
source_update_task = UpdateSourceOperator(task_id='run_update_source', dag=dag, dataset=out_dir)
recipe_task = RunETLOperator(task_id='run_etl', dag=dag,
                             pool='etl',
                             dataset=out_dir)
datapackage_task = GenerateDatapackageOperator(task_id='generate_datapackage', dag=dag,
                                               pool='etl',
                                               dataset=out_dir)
validate_ddf = ValidateDatasetOperator(task_id='validate', dag=dag,
                                       pool='etl',
                                       dataset=out_dir,
                                       logpath=logpath)


def git_push_callback(context):
    ti = context['ti']
    res = ti.xcom_pull(task_ids='git_push', dag_id=dag_id)
    if "git updated." in res:
        slack_report(context, status='new data')


git_push_task = GitPushOperator(task_id='git_push', dag=dag,
                                pool='etl',
                                dataset=out_dir,
                                xcom_push=True,
                                on_success_callback=git_push_callback)
# reseting the branch in case of anything failed
cleanup_task = GitResetOperator(task_id='cleanup', dag=dag, dataset=out_dir, trigger_rule="all_done")


# set dependencies
(dependency_task >>
 checkout_task >>
 source_update_task >>
 recipe_task >>
 datapackage_task >>
 validate_ddf >>
 git_push_task >>
 cleanup_task
)
