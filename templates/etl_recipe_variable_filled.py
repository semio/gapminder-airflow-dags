# -*- coding: utf-8 -*-

"""target_dataset"""

import os.path as osp
from datetime import datetime, timedelta

from airflow.providers.slack.notifications.slack_webhook import (
    send_slack_webhook_notification,
)
from airflow.providers.standard.operators.empty import EmptyOperator
from airflow.providers.standard.operators.python import (
    BranchPythonOperator,
    PythonOperator,
)
from airflow.sdk import DAG, Variable, TaskGroup

from ddf_operators import (
    DependencyDatasetSensor,
    GenerateDatapackageOperator,
    GitCheckoutOperator,
    GitCommitOperator,
    GitPullOperator,
    GitPushOperator,
    GitResetAndGoMasterOperator,
    RunETLOperator,
    SetupVenvOperator,
    UpdateSourceOperator,
    ValidateDatasetOperator,
)

# steps:
# 1. checkout the autogenerated branch, run git pull
# 2. run update_source.py
# 3. run etl.py
# 4. generate datapackage
# 5. validate-ddf
# 6. if there are updates, push

# variables
datasets_dir = Variable.get("datasets_dir")
airflow_home = Variable.get("airflow_home")
airflow_baseurl = Variable.get("airflow_baseurl")

target_dataset = "target_dataset"
depends_on = {"dep_1": "manual", "dep_2": "recipe"}

logpath = osp.join(airflow_home, "validation-log")
out_dir = osp.join(datasets_dir, target_dataset)
dag_id = target_dataset.replace("/", "_")

# Slack notifications
log_url = f"{airflow_baseurl}/dags/{dag_id}/runs/{{{{ dag_run.run_id }}}}/tasks/{{{{ ti.task_id }}}}"
failure_notification = send_slack_webhook_notification(
    slack_webhook_conn_id="slack_webhook",
    text=f"{dag_id}.{{{{ ti.task_id }}}}: failed\nGithub: https://github.com/{target_dataset}\nLogs: {log_url}",
)
success_notification = send_slack_webhook_notification(
    slack_webhook_conn_id="slack_webhook",
    text=f"{dag_id}.{{{{ ti.task_id }}}}: new data\nGithub: https://github.com/{target_dataset}\nLogs: {log_url}",
)

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime.today(),
    "retry_delay": timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'etl',
    "priority_weight": 1,
    # 'end_date': datetime(2016, 1, 1),
    "poke_interval": 60 * 10,  # 10 minutes
    "execution_timeout": timedelta(hours=10),  # 10 hours
    "weight_rule": "absolute",
    "on_failure_callback": [failure_notification],
}

# now define the DAG
etl_type = "recipe"

schedule = "1 0 0 0 0 0"

with DAG(dag_id, default_args=default_args, schedule=schedule) as dag:

    def emit_last_task_run_time(**context):
        """Emit the logical_date as XCom for dependency tracking."""
        ti = context["ti"]
        logical_date = context["logical_date"]
        ti.xcom_push(key="last_task_run_time", value=logical_date)

    emit_run_time = PythonOperator(
        task_id="emit_last_task_run_time",
        python_callable=emit_last_task_run_time,
    )

    checkout_task = GitCheckoutOperator(
        task_id="checkout_autogen_branch", dataset=out_dir, branch="autogenerated"
    )
    git_pull_task = GitPullOperator(task_id="git_pull", dataset=out_dir)
    setup_venv_task = SetupVenvOperator(task_id="setup_venv", dataset=out_dir)
    source_update_task = UpdateSourceOperator(
        task_id="run_update_source", dataset=out_dir
    )
    recipe_task = RunETLOperator(task_id="run_etl", pool="etl", dataset=out_dir)
    datapackage_task = GenerateDatapackageOperator(
        task_id="generate_datapackage", pool="etl", dataset=out_dir
    )
    validate_ddf = ValidateDatasetOperator(
        task_id="validate", pool="etl", dataset=out_dir, logpath=logpath
    )
    git_commit_task = GitCommitOperator(
        task_id="git_commit", pool="etl", dataset=out_dir
    )

    def check_new_commit(**kwargs):
        ti = kwargs["ti"]
        dag_id = ti.dag_id
        res = ti.xcom_pull(task_ids="git_commit", dag_id=dag_id)
        if "git updated" in res:
            return "git_push"
        return "do_nothing"

    branch_task = BranchPythonOperator(
        task_id="check_new_commit", python_callable=check_new_commit
    )

    git_push_task = GitPushOperator(
        task_id="git_push",
        pool="etl",
        dataset=out_dir,
        on_success_callback=[success_notification],
    )

    # reseting the branch in case of anything failed
    cleanup_task = GitResetAndGoMasterOperator(
        task_id="cleanup", dataset=out_dir, trigger_rule="all_done"
    )

    do_nothing = EmptyOperator(task_id="do_nothing")

    # set dependencies with TaskGroup
    if len(depends_on) > 0:
        with TaskGroup("dependency_check") as dependency_group:
            for dep, dep_etl_type in depends_on.items():
                if dep_etl_type == "manual":
                    DependencyDatasetSensor(
                        task_id="wait_for_{}".format(dep).replace("/", "_"),
                        external_dag_id=dep.replace("/", "_"),
                        external_task_id="validate",
                    )
                else:
                    DependencyDatasetSensor(
                        task_id="wait_for_{}".format(dep).replace("/", "_"),
                        external_dag_id=dep.replace("/", "_"),
                        external_task_id="cleanup",
                    )

        emit_run_time >> dependency_group >> checkout_task
    else:
        emit_run_time >> checkout_task

    # etl
    (
        checkout_task
        >> git_pull_task
        >> setup_venv_task
        >> source_update_task
        >> recipe_task
        >> datapackage_task
        >> validate_ddf
        >> git_commit_task
    )

    # commit
    git_commit_task >> branch_task
    branch_task >> git_push_task >> cleanup_task
    branch_task >> do_nothing >> cleanup_task
