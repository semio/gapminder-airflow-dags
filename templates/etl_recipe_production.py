# -*- coding: utf-8 -*-

"""{{ name }}"""

import os.path as osp
from datetime import datetime, timedelta

from airflow.hooks.base import BaseHook
from airflow.providers.standard.operators.bash import BashOperator
from airflow.providers.standard.operators.python import PythonOperator
from airflow.sdk import DAG, Variable, TaskGroup

from ddf_operators import (
    DependencyDatasetSensor,
    GitMergeOperator,
    SlackReportOperator,
)

# steps:
# - merge autogenerated branch to master
# - git push
# - upload to S3

# variables
target_dataset = '{{ name }}'

datasets_dir = Variable.get('datasets_dir')
airflow_home = Variable.get('airflow_home')

endpoint = BaseHook.get_connection('slack_connection').password
airflow_baseurl = BaseHook.get_connection('airflow_web').host

dag_id = target_dataset.replace('/', '_') + "_production"
out_dir = osp.join(datasets_dir, target_dataset)


def slack_report(context):
    reporter = SlackReportOperator(
        endpoint=endpoint,
        status='failed',
        airflow_baseurl=airflow_baseurl,
    )
    context['target_dataset'] = '{{ name }}'
    reporter.execute(context)


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': {{ datetime }},
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'etl',
    'priority_weight': {{ priority }},
    # 'end_date': datetime(2016, 1, 1),
    'poke_interval': 60 * 10,  # 10 minutes
    'execution_timeout': timedelta(hours=10),  # 10 hours
    'weight_rule': 'absolute',
    'on_failure_callback': slack_report,
}

# now define the DAG
schedule = '@once'

with DAG(dag_id, default_args=default_args, schedule=schedule) as dag:

    def emit_last_task_run_time(**context):
        """Emit the logical_date as XCom for dependency tracking."""
        ti = context['ti']
        logical_date = context['logical_date']
        ti.xcom_push(key='last_task_run_time', value=logical_date)

    emit_run_time = PythonOperator(
        task_id='emit_last_task_run_time',
        python_callable=emit_last_task_run_time,
    )

    # dependency check with TaskGroup
    with TaskGroup('dependency_check') as dependency_group:
        # prevent running when the other DAG for autogenerated branch is running
        DependencyDatasetSensor(
            task_id='check_other_dag',
            external_dag_id=target_dataset.replace('/', '_'),
            external_task_id='validate',
        )

    git_merge_task = GitMergeOperator(
        task_id='merge_into_master',
        dataset=out_dir,
        base='master',
        head='autogenerated',
    )

    {% raw %}
    git_push_command = '''\
set -eu

cd {{ params.dataset_dir }}

git push

'''
    {% endraw %}
    git_push_task = BashOperator(
        task_id='git_push',
        bash_command=git_push_command,
        params={'dataset_dir': out_dir},
    )

    # set dependencies
    (
        emit_run_time
        >> dependency_group
        >> git_merge_task
        >> git_push_task
    )
