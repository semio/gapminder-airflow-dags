# -*- coding: utf-8 -*-

import json
import logging
import os
import os.path as osp
from dataclasses import dataclass
from urllib.parse import urlencode, urljoin

import requests

import airflow_client.client
from airflow_client.client.api import task_instance_api

from airflow.providers.http.operators.http import HttpOperator
from airflow.providers.standard.operators.bash import BashOperator
from airflow.providers.standard.operators.python import PythonOperator
from airflow.providers.standard.sensors.external_task import ExternalTaskSensor
from airflow.sdk import Variable
from airflow.sdk import Context

from ddf_utils.io import dump_json
from ddf_utils.package import get_datapackage


@dataclass
class AirflowAccessTokenResponse:
    access_token: str
    token_type: str = "Bearer"


def get_airflow_client_access_token(
    host: str,
    username: str,
    password: str,
) -> str:
    """Retrieve an access token from Airflow API."""
    url = f"{host}/auth/token"
    payload = {
        "username": username,
        "password": password,
    }
    headers = {"Content-Type": "application/json"}
    response = requests.post(url, json=payload, headers=headers)
    if response.status_code != 201:
        raise RuntimeError(f"Failed to get access token: {response.status_code} {response.text}")
    response_success = AirflowAccessTokenResponse(**response.json())
    return response_success.access_token


log = logging.getLogger(__name__)


class GenerateDatapackageOperator(PythonOperator):
    def __init__(self, dataset, *args, **kwargs):
        def _gen_dp(d):
            dp = get_datapackage(d, update=True)
            dump_json(osp.join(dataset, "datapackage.json"), dp)

        super().__init__(python_callable=_gen_dp, op_args=[dataset], *args, **kwargs)


class RunETLOperator(BashOperator):
    def __init__(self, dataset, *args, **kwargs):
        # TODO: think about how to handle datasets_dir here
        bash_command = """\
        set -eu
        export DATASETS_DIR={{ params.datasets_dir }}
        cd {{ params.dataset }}
        ddf cleanup --exclude icon.png ddf .

        cd etl/scripts/
        python3 etl.py
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "datasets_dir": Variable.get("datasets_dir")},
            env={"GSPREAD_PANDAS_CONFIG_DIR": Variable.get("GSPREAD_PANDAS_CONFIG_DIR")},
            *args,
            **kwargs,
        )


class UpdateSourceOperator(BashOperator):
    def __init__(self, dataset, *args, **kwargs):
        bash_command = """\
        set -eu
        export DATASETS_DIR={{ params.datasets_dir }}
        cd {{ params.dataset }}

        cd etl/scripts/
        if [ -f update_source.py ]; then
            python3 update_source.py
            echo "updated source."
        else
            echo "no updater script"
        fi
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "datasets_dir": Variable.get("datasets_dir")},
            env={"GSPREAD_PANDAS_CONFIG_DIR": Variable.get("GSPREAD_PANDAS_CONFIG_DIR")},
            *args,
            **kwargs,
        )


class GitCheckoutOperator(BashOperator):
    def __init__(self, dataset, branch, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        git checkout {{ params.branch }}
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "branch": branch},
            *args,
            **kwargs,
        )


class GitPullOperator(BashOperator):
    def __init__(self, dataset, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        git pull
        git submodule update --merge
        """
        super().__init__(bash_command=bash_command, params={"dataset": dataset}, *args, **kwargs)


class GitMergeOperator(BashOperator):
    def __init__(self, dataset, head, base, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        git checkout {{ params.base }}
        git merge {{ params.head }}
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "head": head, "base": base},
            *args,
            **kwargs,
        )


class GitPushOperator(BashOperator):
    """Check if there are updates, And push when necessary"""

    def __init__(self, dataset, push_all=False, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        {% if params.push_all is sameas true %}
        git checkout autogenerated
        git push -u origin
        git checkout develop
        git push -u origin
        git checkout master
        git push -u origin
        {% else %}
        git push -u origin
        {% endif %}
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "push_all": push_all},
            *args,
            **kwargs,
        )


class GitCommitOperator(BashOperator):
    """Check if there are updates, And make a commit when necessary.

    It will also push xcom when there is new commit.
    """

    def __init__(self, dataset, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        if [[ $(git status -s | grep -e '^[? ][?D]' | head -c1 | wc -c) -ne 0 ]]; then
            git add .
            git commit -m "auto generated dataset"
            echo "git updated"
        else
            HAS_UPDATE=0
            for f in $(git diff --name-only | grep -v datapackage.json); do
                if [[ $(git diff $f | tail -n +3 | grep -e "^[++|\-\-]" | head -c1 | wc -c) -ne 0 ]]; then
                    HAS_UPDATE=1
                    git add $f
                fi
            done
            if [[ $HAS_UPDATE -eq 1 ]]; then
                git add datapackage.json
                git commit -m "auto generated dataset"
                echo "git updated"
            else
                echo "nothing new"
            fi
        fi
        """
        super().__init__(bash_command=bash_command, params={"dataset": dataset}, *args, **kwargs)


class GitResetOperator(BashOperator):
    def __init__(self, dataset, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        export COMMIT=`git for-each-ref --format='%(upstream:short)' "$(git symbolic-ref -q HEAD)"`
        git reset --hard $COMMIT
        git clean -dfx
        """
        super().__init__(bash_command=bash_command, params={"dataset": dataset}, *args, **kwargs)


class GitResetAndGoMasterOperator(BashOperator):
    """reset current head and then checkout master branch"""

    def __init__(self, dataset, *args, **kwargs):
        bash_command = """\
        set -eu
        cd {{ params.dataset }}
        export BRANCH=`git rev-parse --abbrev-ref HEAD`
        case $BRANCH in
            master | develop)
                git reset --hard origin/$BRANCH
                git checkout autogenerated
                git reset --hard origin/autogenerated
                ;;
            autogenerated)
                git reset --hard origin/autogenerated
                ;;
        esac
        git clean -dfx
        git checkout master
        """
        super().__init__(bash_command=bash_command, params={"dataset": dataset}, *args, **kwargs)


class ValidateDatasetOperator(BashOperator):
    def __init__(self, dataset, logpath, *args, **kwargs):
        bash_command = """\
        export NODE_OPTIONS="--max-old-space-size=7000"
        cd {{ params.dataset }}
        validate-ddf-ng --no-warning ./
        if [ $? -eq 0 ]
        then
            sleep 2
            echo "validation succeed."
            exit 0
        else
            sleep 2
            echo "validation failed."
            exit 1
        fi
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "logpath": logpath},
            *args,
            **kwargs,
        )


class ValidateDatasetDependOnGitOperator(BashOperator):
    def __init__(self, dataset, logpath, *args, **kwargs):
        bash_command = """\
        cd {{ params.dataset }}
        LASTGITCOMMITDATE=`git log -1 --format=%at`
        YESTERDAY=`date -d "yesterday" "+%s"`

        run () {
            DT=`date "+%Y-%m-%dT%H-%M-%S"`
            VALIDATE_OUTPUT="validation-$DT.log"
            echo "logfile: $VALIDATE_OUTPUT"
            RES=`validate-ddf ./ --exclude-tags "WARNING TRANSLATION" --silent --heap 8192 --multithread`
            if [ $? -eq 0 ]
            then
                sleep 2
                echo "validation succeed."
                exit 0
            else
                sleep 2
                echo $RES > $VALIDATE_OUTPUT
                if [ `cat $VALIDATE_OUTPUT | wc -c` -ge 5 ]
                then
                    echo "validation not successful, moving the log file..."
                    LOGPATH="{{ params.logpath }}/`basename {{ params.dataset }}`"
                    if [ ! -d $LOGPATH ]; then
                        mkdir $LOGPATH
                    fi
                    mv $VALIDATE_OUTPUT $LOGPATH
                    exit 1
                else
                    echo "ddf-validation failed but no output."
                    rm $VALIDATE_OUTPUT
                    exit 1
                fi
            fi
        }

        if [ $LASTGITCOMMITDATE -ge $YESTERDAY ]
        then
            echo "there is new updates, need to validate"
            run
        else
            echo "no updates."
            exit 0
        fi
        """
        super().__init__(
            bash_command=bash_command,
            params={"dataset": dataset, "logpath": logpath},
            *args,
            **kwargs,
        )


def _get_last_task_instance_date(
    dag_id: str,
    task_id: str,
):
    """Get the logical date of the last task instance using Airflow REST API.

    Args:
        dag_id: The DAG ID to query
        task_id: The task ID to query

    Returns:
        A function that can be used as execution_date_fn for ExternalTaskSensor

    Environment variables:
        AIRFLOW_BASEURL: The base URL of Airflow (e.g., http://localhost:8080)
        AIRFLOW_API_USER: The API username for authentication
        AIRFLOW_API_PASSWORD: The API password for authentication
    """

    def _get_execution_date(logical_date, **context):
        host = os.environ.get("AIRFLOW_BASEURL", "http://localhost:8080")
        api_url = f"{host}/api/v2"
        username = os.environ["AIRFLOW_API_USER"]
        password = os.environ["AIRFLOW_API_PASSWORD"]

        configuration = airflow_client.client.Configuration(host=api_url)
        configuration.access_token = get_airflow_client_access_token(
            host=host,
            username=username,
            password=password,
        )

        with airflow_client.client.ApiClient(configuration) as api_client:
            api_instance = task_instance_api.TaskInstanceApi(api_client)

            try:
                # Get task instances for the DAG, ordered by logical_date descending
                # No state filter - we want the most recent run regardless of state
                response = api_instance.get_task_instances(
                    dag_id=dag_id,
                    dag_run_id="~",  # Match all dag runs
                    limit=1,
                    order_by="-logical_date",
                )

                # Filter by task_id and find the most recent one
                for ti in response.task_instances:
                    if ti.task_id == task_id:
                        return [ti.logical_date]

                log.warning(f"No task instance found for {dag_id}.{task_id}")
                return []

            except airflow_client.client.ApiException as e:
                log.error(f"Error querying Airflow API: {e}")
                raise

    return _get_execution_date


class DependencyDatasetSensor(ExternalTaskSensor):
    """Sensor that waits for the most recent run of an external task to succeed.

    This sensor extends ExternalTaskSensor and uses the Airflow REST API to find
    the most recent task instance (regardless of its current state), then waits
    for that task to reach a successful state.

    Args:
        external_dag_id: The DAG ID of the external task to wait for
        external_task_id: The task ID of the external task to wait for
        allowed_states: States considered as successful (default: ['success'])
        failed_states: States considered as failed (default: ['failed'])
        **kwargs: Additional arguments passed to ExternalTaskSensor

    Environment variables:
        AIRFLOW_BASEURL: The base URL of Airflow (e.g., http://localhost:8080)
        AIRFLOW_API_USER: The API username for authentication
        AIRFLOW_API_PASSWORD: The API password for authentication
    """

    def __init__(
        self,
        external_dag_id: str,
        external_task_id: str,
        allowed_states: list[str] | None = None,
        failed_states: list[str] | None = None,
        *args,
        **kwargs,
    ):
        # Create the execution_date_fn that queries the API for the last task run
        execution_date_fn = _get_last_task_instance_date(
            dag_id=external_dag_id,
            task_id=external_task_id,
        )

        super().__init__(
            external_dag_id=external_dag_id,
            external_task_id=external_task_id,
            execution_date_fn=execution_date_fn,
            allowed_states=allowed_states or ["success"],
            failed_states=failed_states or ["failed"],
            *args,
            **kwargs,
        )


class NotifyWaffleServerOperator(BashOperator):
    """Fake a slack command to load the dataset in waffle server"""

    def __init__(self, dataset, *args, **kwargs):
        base_name = dataset.split("/")[-1]
        conf = Variable.get("automatic_ws_datasets_conf", deserialize_json=True)
        if dataset in conf.keys():
            branch = conf[dataset]["branch"]
            ws_dataset_id = conf[dataset]["ws_dataset_id"]
        else:
            branch = "master"
            ws_dataset_id = "-".join(base_name.split("--")[1:]) + "-" + branch
        text = f"-N {ws_dataset_id} --publish https://github.com/{dataset}.git {branch}"

        bash_command = """\
        set -eu
        curl -d 'token=foo' -d 'command=/bwload' --data-urlencode 'text={{ params.text }}' http://35.228.158.102/slack/
        """

        super().__init__(bash_command=bash_command, params={"text": text}, *args, **kwargs)


class SlackReportOperator(HttpOperator):
    """Operator to report a message to slack with default buttons"""

    def __init__(self, status, airflow_baseurl, *args, **kwargs):
        """report status of task in slack.

        status: task status, possible values are same as task status in airflow
        """
        super().__init__(*args, **kwargs)
        self.status = status
        self.airflow_baseurl = airflow_baseurl

    def execute(self, context: Context):
        # overwrite self.data to custom message
        dag_id = context["dag_run"].dag_id  # type: ignore
        task_id = context["ti"].task_id  # type: ignore
        ts = context["ts"]  # type: ignore
        dataset = context.get("target_dataset", None)

        text = f"{dag_id}.{task_id}: {self.status}"
        log_url = osp.join(self.airflow_baseurl, "log?")
        log_url = log_url + urlencode(
            {
                "task_id": task_id,
                "dag_id": dag_id,
                "execution_date": ts,
                "format": "json",
            }
        )

        blocks = [
            {"type": "section", "text": {"type": "mrkdwn", "text": text}},
            {
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {"type": "plain_text", "text": "Show log"},
                        "url": log_url,
                    }
                ],
            },
        ]

        if dataset:
            git_url = urljoin("https://github.com", dataset)
            blocks[1]["elements"].append(
                {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "Github"},
                    "url": git_url,
                }
            )

        data = {"blocks": blocks}
        data = json.dumps(data)

        self.data = data
        self.log.info(data)
        super().execute(context)
