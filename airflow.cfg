# for a complete reference of configurable options, see
# https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html

[core]

# This defines the maximum number of task instances that can run concurrently per scheduler in
# Airflow, regardless of the worker count. Generally this value, multiplied by the number of
# schedulers in your cluster, is the maximum number of task instances with the running
# state in the metadata database. The value must be larger or equal 1.
#
# Variable: AIRFLOW__CORE__PARALLELISM
#
parallelism = 8

# The maximum number of task instances allowed to run concurrently in each dag run.
# This is also configurable per-dag with ``max_active_tasks``,
# which is defaulted as ``[core] max_active_tasks_per_dag``.
#
# An example scenario when this would be useful is when you want to stop a new dag run with an early
# start date from stealing all the executor slots in a cluster.
#
# Variable: AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG
#
max_active_tasks_per_dag = 3

# Are DAGs paused by default at creation
#
# Variable: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
#
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG. The scheduler will not create more DAG runs
# if it reaches the limit. This is configurable at the DAG level with ``max_active_runs``,
# which is defaulted as ``[core] max_active_runs_per_dag``.
#
# Variable: AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG
#
max_active_runs_per_dag = 1

# Whether to load the DAG examples that ship with Airflow. It's good to
# get started, but you probably want to set this to ``False`` in a production
# environment
#
# Variable: AIRFLOW__CORE__LOAD_EXAMPLES
#
load_examples = False

# The number of retries each task is going to have by default. Can be overridden at dag or task level.
#
# Variable: AIRFLOW__CORE__DEFAULT_TASK_RETRIES
#
default_task_retries = 3

[database]

# The SQLAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
#
# Variable: AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE
#
sql_alchemy_pool_size = 3

[email]
# Whether email alerts should be sent when a task is retried
#
# Variable: AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_RETRY
#
default_email_on_retry = True

# Whether email alerts should be sent when a task failed
#
# Variable: AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_FAILURE
#
default_email_on_failure = True

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
#
# Variable: AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC
#
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC
#
scheduler_heartbeat_sec = 5

# The frequency (in seconds) at which the LocalTaskJob should send heartbeat signals to the
# scheduler to notify it's still alive. If this value is set to 0, the heartbeat interval will default
# to the value of ``[scheduler] task_instance_heartbeat_timeout``.
#
# Variable: AIRFLOW__SCHEDULER__TASK_INSTANCE_HEARTBEAT_SEC
#
task_instance_heartbeat_sec = 0

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
#
# Variable: AIRFLOW__SCHEDULER__NUM_RUNS
#
num_runs = -1

# Controls how long the scheduler will sleep between loops, but if there was nothing to do
# in the loop. i.e. if it scheduled something then it will start the next loop
# iteration straight away.
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_IDLE_SLEEP_TIME
#
scheduler_idle_sleep_time = 20

# Turn on scheduler catchup by setting this to ``True``.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is ``False``,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
#
# Variable: AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT
#
catchup_by_default = False

# This determines the number of task instances to be evaluated for scheduling
# during each scheduler loop.
# Set this to 0 to use the value of ``[core] parallelism``
#
# Variable: AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY
#
max_tis_per_query = 0

[triggerer]
# How many triggers a single Triggerer will run at once, by default.
#
# Variable: AIRFLOW__TRIGGERER__CAPACITY
#
capacity = 200

# How often to heartbeat the Triggerer job to ensure it hasn't been killed.
#
# Variable: AIRFLOW__TRIGGERER__JOB_HEARTBEAT_SEC
#
job_heartbeat_sec = 5

# If the last triggerer heartbeat happened more than ``[triggerer] triggerer_health_check_threshold``
# ago (in seconds), triggerer is considered unhealthy.
# This is used by the health check in the **/health** endpoint and in ``airflow jobs check`` CLI
# for TriggererJob.
#
# Variable: AIRFLOW__TRIGGERER__TRIGGERER_HEALTH_CHECK_THRESHOLD
#
triggerer_health_check_threshold = 30

[dag_processor]
# How often (in seconds) to refresh, or look for new files, in a DAG bundle.
#
# Variable: AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL
#
refresh_interval = 300

# The DAG processor can run multiple processes in parallel to parse dags.
# This defines how many processes will run.
#
# Variable: AIRFLOW__DAG_PROCESSOR__PARSING_PROCESSES
#
parsing_processes = 2

[standard]
# Options for the standard provider operators.

# Which python tooling should be used to install the virtual environment.
#
# The following options are available:
# - ``auto``: Automatically select, use ``uv`` if available, otherwise use ``pip``.
# - ``pip``: Use pip to install the virtual environment.
# - ``uv``: Use uv to install the virtual environment. Must be available in environment PATH.
#
# Example: venv_install_method = uv
#
# Variable: AIRFLOW__STANDARD__VENV_INSTALL_METHOD
#
venv_install_method = uv
