# -*- coding: utf-8 -*-

import logging
import os
import os.path as osp
import subprocess
from datetime import datetime, timedelta

from airflow.models import Variable
from airflow.providers.standard.operators.bash import BashOperator
from airflow.sdk import dag, task
from jinja2 import Environment, FileSystemLoader

from ddf_utils.chef.api import Chef

logger = logging.getLogger(__name__)

datasets_dir = Variable.get("datasets_dir")
airflow_home = Variable.get("airflow_home")
gcs_datasets = [x.strip() for x in Variable.get("with_production").split("\n")]
auto_datasets = [x.strip() for x in Variable.get("automatic_datasets").split("\n")]
check_source_datasets = [
    x.strip() for x in Variable.get("check_source_datasets").split("\n")
]
custom_schedule = Variable.get("custom_schedule", deserialize_json=True)

git_checkmaster_template = """\
set -eu

DIR={{ params.datasetpath }}

cd $DIR/open-numbers

for i in `ls`; do
  cd "$i"
  echo "checking out master for: $i"

  if [[ $(git branch -a | grep master | head -c1 | wc -c) -ne 0 ]]
  then
      git checkout master
  fi
  cd ..
done

"""

git_checkout_dev_template = """\
set -eu

DIR={{ params.datasetpath }}

cd $DIR/open-numbers

for i in `ls`; do
  cd "$i"
  echo "checking out dev branch for: $i"

  if [[ $(git branch -a | grep origin/autogenerated | head -c1 | wc -c) -ne 0 ]]
  then
      git checkout autogenerated
  elif [[ $(git branch -a | grep origin/develop | head -c1 | wc -c) -ne 0 ]]
  then
      git checkout develop
  elif [[ $(git branch -a | grep master | head -c1 | wc -c) -ne 0 ]]
  then
      git checkout master
  fi
  cd ..
done

"""


def check_etl_type():
    current_datasets = os.listdir(osp.join(datasets_dir, "open-numbers"))
    datasets_types = {
        "open-numbers/" + k: list(_get_dataset_type("open-numbers/" + k))
        for k in current_datasets
    }

    return {"current_datasets": datasets_types}


def _get_dataset_type(dataset):
    dataset_path = osp.join(datasets_dir, dataset)
    etl_dir = osp.join(dataset_path, "etl/scripts")

    out = subprocess.run(["ddf", "etl_type", "-d", etl_dir], stdout=subprocess.PIPE)
    if out.returncode != 0:
        logging.info("command did not return successfully. fall back to manual")
        return ["manual", ""]
    return out.stdout.decode("utf-8").split("\n")[-2].split(",")


def _get_denpendencies(dataset, all_datasets, include_indirect=True):
    try:
        etl_type, fn = all_datasets[dataset]
    except KeyError:  # not open_numbers datasets
        return list()

    if etl_type == "recipe":
        dataset_path = osp.join(datasets_dir, dataset)
        etl_dir = osp.join(dataset_path, "etl/scripts")
        recipe = osp.join(etl_dir, fn)
        logging.info("using recipe file: " + fn)
        chef = Chef.from_recipe(recipe, ddf_dir=datasets_dir)
        dependencies = list()
        for i in chef.ingredients:
            if i.dataset is not None:
                dependencies.append(i.dataset)
                if include_indirect:
                    for d in _get_denpendencies(
                        i.dataset, all_datasets, include_indirect=True
                    ):
                        dependencies.append(d)
        dependencies = list(set(dependencies))
        logging.info("dependencies: {}".format(dependencies))
        return dependencies
    else:
        return list()


def refresh_dags(current_datasets):
    """add/modify dags"""
    current = current_datasets["current_datasets"]

    env = Environment(loader=FileSystemLoader(osp.join(airflow_home, "templates")))

    def refresh_normal_dag(dataset):
        # 1. get all dependencies from etl scripts
        # 2. re-generate the DAG, replace the old one
        dependencies = _get_denpendencies(dataset, current)
        etl_type, _ = current[dataset]

        if etl_type == "recipe":
            now = datetime.utcnow() - timedelta(days=1)
            if dataset in auto_datasets:
                template = env.get_template("etl_recipe_auto.py")
            else:
                template = env.get_template("etl_recipe.py")
            p = 100 - len(dependencies)  # The more dependencies, the less priority
        elif etl_type == "python":
            now = datetime.utcnow() - timedelta(days=7)
            if dataset in auto_datasets:
                template = env.get_template("etl_recipe_auto.py")
            elif dataset in check_source_datasets:
                template = env.get_template("check_source_only.py")
            else:
                template = env.get_template("etl_recipe.py")
            p = 100
        else:
            now = datetime.utcnow() - timedelta(days=1)
            template = env.get_template("manual_update.py")
            p = 100

        # config schedule
        schedule = custom_schedule.get(dataset, None)
        if not schedule:
            if etl_type == "recipe":
                schedule = "0 12 * * *"  # recipe datasets: 12:00 everyday
            elif etl_type == "python":
                schedule = "0 1 * * 0"  # source datasets: 1:00 every Sunday
            else:
                schedule = "30 0 * * *"  # manual datasets: 0:30 everyday

        dt_str = "datetime({}, {}, {})".format(now.year, now.month, now.day)

        dag_name = dataset.replace("/", "_")
        dag_path = osp.join(airflow_home, "dags", "datasets", dag_name)

        # adding dependency checking dags, but don't consider non-open_numbers ones
        direct_deps = _get_denpendencies(dataset, current, include_indirect=False)
        direct_deps = list(filter(lambda x: x.startswith("open-numbers"), direct_deps))
        direct_deps = {d: current[d][0] for d in direct_deps}

        with open(dag_path + ".py", "w") as f:
            f.write(
                template.render(
                    name=dataset,
                    datetime=dt_str,
                    schedule=schedule,
                    priority=p,
                    etl_type=etl_type,
                    dependencies=direct_deps,
                )
            )
            f.close()

    def refresh_production_dag(dataset):
        now = datetime.utcnow() - timedelta(days=1)
        template = env.get_template("etl_recipe_production.py")
        p = 100
        dt_str = "datetime({}, {}, {})".format(now.year, now.month, now.day)

        dag_name = dataset.replace("/", "_") + "_production"
        dag_path = osp.join(airflow_home, "dags", "datasets", dag_name)

        with open(dag_path + ".py", "w") as f:
            f.write(template.render(name=dataset, datetime=dt_str, priority=p))
            f.close()

    for ds in current.keys():
        logging.info("checking {}".format(ds))
        refresh_normal_dag(ds)

        if ds in gcs_datasets:
            refresh_production_dag(ds)


REMOVE_DAG_COMMAND = f"""\
set -eu

cd {osp.join(airflow_home, "dags", "datasets")}

rm -f ./*.py || true

"""

REPARSE_DAGS_COMMAND = """\
set -eu

airflow dags reserialize

"""


@dag(
    dag_id="refresh_dags",
    schedule="@once",
    start_date=datetime(2018, 11, 25),
    catchup=False,
    default_args={
        "owner": "airflow",
        "depends_on_past": False,
        "retry_delay": timedelta(minutes=5),
        "priority_weight": 200,
        "weight_rule": "absolute",
    },
)
def refresh_dags_dag():
    @task
    def check_etl_type_task():
        return check_etl_type()

    @task
    def refresh_dags_task(current_datasets):
        refresh_dags(current_datasets)

    checkout_dev_branch = BashOperator(
        task_id="checkout_dev_branches",
        bash_command=git_checkout_dev_template,
        params={"datasetpath": datasets_dir},
        retries=3,
        retry_delay=timedelta(seconds=10),
    )

    etl_type_result = check_etl_type_task()

    checkout_master = BashOperator(
        task_id="checkout_master_branches",
        bash_command=git_checkmaster_template,
        params={"datasetpath": datasets_dir},
        retries=3,
        retry_delay=timedelta(seconds=10),
        trigger_rule="all_done",
    )

    remove_dags = BashOperator(
        task_id="remove_dags",
        bash_command=REMOVE_DAG_COMMAND,
    )

    refresh = refresh_dags_task(etl_type_result)

    reparse_dags = BashOperator(
        task_id="reparse_dags",
        bash_command=REPARSE_DAGS_COMMAND,
    )

    (
        checkout_dev_branch
        >> etl_type_result
        >> checkout_master
        >> remove_dags
        >> refresh
        >> reparse_dags
    )


refresh_dags_dag()
